diff --git a/app/base/utils/mongodb.py b/app/base/utils/mongodb.py
index 25fefc1..8cab63c 100644
--- a/app/base/utils/mongodb.py
+++ b/app/base/utils/mongodb.py
@@ -1,9 +1,20 @@
-from pymongo.mongo_client import MongoClient
-from pymongo.server_api import ServerApi
+"""
+MongoDB client â€” module-level singleton using motor (async).
+
+Instead of creating a new client on every request (the old behaviour),
+this module creates a single AsyncIOMotorClient at import time and caches
+the resolved Database object.  Callers continue to use connect_mongodb()
+for backward compatibility; it now simply returns the cached Database.
+"""
+
+from __future__ import annotations
+
 import os
-from dotenv import load_dotenv
 import certifi
+from dotenv import load_dotenv
 from motor.motor_asyncio import AsyncIOMotorClient
+from pymongo.server_api import ServerApi
+
 load_dotenv()
 
 COLLECTIONS = {
@@ -14,20 +25,29 @@ COLLECTIONS = {
     "ticker_infos": "ticker_infos",
 }
 
+# ---------------------------------------------------------------------------
+# Singleton client â€” created once at module import time.
+# ---------------------------------------------------------------------------
+_MONGO_URI = (
+    f"mongodb+srv://{os.environ.get('MONGO_USER')}:{os.environ.get('MONGO_PASSWORD')}"
+    f"@develop.dkur4lg.mongodb.net/?retryWrites=true&w=majority"
+)
+
+_client: AsyncIOMotorClient = AsyncIOMotorClient(
+    _MONGO_URI,
+    server_api=ServerApi("1"),
+    tlsCAFile=certifi.where(),
+)
+
+_db_name: str = "production" if os.environ.get("ENV") == "production" else "develop"
+_database = _client[_db_name]
+
+
 async def connect_mongodb():
-  uri = f"mongodb+srv://{os.environ['MONGO_USER']}:{os.environ['MONGO_PASSWORD']}@develop.dkur4lg.mongodb.net/?retryWrites=true&w=majority"
-
-  # Create a new client and connect to the server
-  # client = MongoClient(uri, server_api=ServerApi('1'))
-  client = AsyncIOMotorClient(uri, server_api=ServerApi('1'), tlsCAFile=certifi.where())
-
-  # Send a ping to confirm a successful connection
-  try:
-      client.admin.command('ping')
-      print("Pinged your deployment. You successfully connected to MongoDB!")
-  except Exception as e:
-      print(e)
-
-  if (os.environ['ENV'] == 'production'):
-    return client['production']
-  return client['develop']
\ No newline at end of file
+    """
+    Return the cached Motor database instance.
+
+    Kept as an async function for full backward compatibility with all callers
+    that do ``db = await connect_mongodb()``.
+    """
+    return _database
\ No newline at end of file
diff --git a/app/db/__init__.py b/app/db/__init__.py
new file mode 100644
index 0000000..1ea0f0d
--- /dev/null
+++ b/app/db/__init__.py
@@ -0,0 +1 @@
+# app/db â€” SQLAlchemy async database layer
diff --git a/app/db/models.py b/app/db/models.py
new file mode 100644
index 0000000..f10a28d
--- /dev/null
+++ b/app/db/models.py
@@ -0,0 +1,152 @@
+"""
+SQLAlchemy ORM models mapping to the existing Postgres tables in Supabase.
+
+Tables reflected:
+  - backtest_stats
+  - trade_actions
+  - unique_strategies  (read-only view / table)
+"""
+
+from __future__ import annotations
+
+import uuid
+from datetime import datetime
+
+from sqlalchemy import TIMESTAMP, BigInteger, Boolean, Double, ForeignKey, Integer, Text
+from sqlalchemy.dialects.postgresql import TIMESTAMP as TIMESTAMPTZ
+from sqlalchemy.dialects.postgresql import UUID
+from sqlalchemy.orm import Mapped, mapped_column, relationship
+
+from app.db.postgres import Base
+
+# ---------------------------------------------------------------------------
+# backtest_stats
+# ---------------------------------------------------------------------------
+
+
+class BacktestStat(Base):
+    __tablename__ = "backtest_stats"
+
+    id: Mapped[int] = mapped_column(
+        Integer,
+        primary_key=True,
+        autoincrement=True,
+    )
+
+    # Core identifiers
+    ticker: Mapped[str | None] = mapped_column(Text, nullable=True)
+    strategy: Mapped[str | None] = mapped_column(Text, nullable=True)
+    period: Mapped[str | None] = mapped_column(Text, nullable=True)
+    interval: Mapped[str | None] = mapped_column(Text, nullable=True)
+    ref_id: Mapped[str | None] = mapped_column(Text, nullable=True)
+
+    # Performance metrics
+    return_percentage: Mapped[float | None] = mapped_column(Double, nullable=True)
+    return_annualized: Mapped[float | None] = mapped_column(Double, nullable=True)
+    buy_and_hold_return: Mapped[float | None] = mapped_column(Double, nullable=True)
+    sharpe_ratio: Mapped[float | None] = mapped_column(Double, nullable=True)
+    sortino_ratio: Mapped[float | None] = mapped_column(Double, nullable=True)
+    calmar_ratio: Mapped[float | None] = mapped_column(Double, nullable=True)
+    volatility_annualized: Mapped[float | None] = mapped_column(Double, nullable=True)
+
+    # Equity
+    final_equity: Mapped[float | None] = mapped_column(Double, nullable=True)
+    peak_equity: Mapped[float | None] = mapped_column(Double, nullable=True)
+
+    # Drawdown
+    max_drawdown_percentage: Mapped[float | None] = mapped_column(Double, nullable=True)
+    average_drawdown_percentage: Mapped[float | None] = mapped_column(Double, nullable=True)
+    max_drawdown_duration: Mapped[str | None] = mapped_column(Text, nullable=True)
+    average_drawdown_duration: Mapped[str | None] = mapped_column(Text, nullable=True)
+
+    # Trade stats
+    trade_count: Mapped[int | None] = mapped_column(Integer, nullable=True)
+    win_rate: Mapped[float | None] = mapped_column(Double, nullable=True)
+    best_trade: Mapped[float | None] = mapped_column(Double, nullable=True)
+    worst_trade: Mapped[float | None] = mapped_column(Double, nullable=True)
+    avg_trade: Mapped[float | None] = mapped_column(Double, nullable=True)
+    max_trade_duration: Mapped[str | None] = mapped_column(Text, nullable=True)
+    average_trade_duration: Mapped[str | None] = mapped_column(Text, nullable=True)
+    profit_factor: Mapped[float | None] = mapped_column(Double, nullable=True)
+    exposure_time_percentage: Mapped[float | None] = mapped_column(Double, nullable=True)
+
+    # Duration
+    start_time: Mapped[str | None] = mapped_column(Text, nullable=True)
+    end_time: Mapped[str | None] = mapped_column(Text, nullable=True)
+    duration: Mapped[str | None] = mapped_column(Text, nullable=True)
+
+    # Strategy params
+    tpsl_ratio: Mapped[float | None] = mapped_column(Double, nullable=True)
+    sl_coef: Mapped[float | None] = mapped_column(Double, nullable=True)
+    tp_coef: Mapped[float | None] = mapped_column(Double, nullable=True)
+
+    # Notifications
+    notifications_on: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
+
+    # The backtest HTML (deflated + base64)
+    html: Mapped[str | None] = mapped_column(Text, nullable=True)
+
+    # Timestamps
+    updated_at: Mapped[datetime | None] = mapped_column(TIMESTAMPTZ, nullable=True)
+    last_optimized_at: Mapped[datetime | None] = mapped_column(TIMESTAMPTZ, nullable=True)
+
+    # Relationship
+    trade_actions: Mapped[list[TradeAction]] = relationship(
+        "TradeAction", back_populates="backtest_stat", lazy="select"
+    )
+
+
+# ---------------------------------------------------------------------------
+# trade_actions
+# ---------------------------------------------------------------------------
+
+
+class TradeAction(Base):
+    __tablename__ = "trade_actions"
+
+    id: Mapped[int] = mapped_column(
+        BigInteger,
+        primary_key=True,
+        autoincrement=True,
+    )
+
+    # Foreign key to backtest_stats
+    backtest_id: Mapped[int | None] = mapped_column(
+        Integer,
+        ForeignKey("backtest_stats.id", ondelete="SET NULL"),
+        nullable=True,
+    )
+
+    # Trade details
+    datetime: Mapped[datetime | None] = mapped_column(TIMESTAMP, nullable=True)
+    trade_action: Mapped[str | None] = mapped_column(Text, nullable=True)
+    entry_price: Mapped[float | None] = mapped_column(Double, nullable=True)
+    price: Mapped[float | None] = mapped_column(Double, nullable=True)
+    sl: Mapped[float | None] = mapped_column(Double, nullable=True)
+    tp: Mapped[float | None] = mapped_column(Double, nullable=True)
+    size: Mapped[float | None] = mapped_column(Double, nullable=True)
+
+    # Relationship
+    backtest_stat: Mapped[BacktestStat | None] = relationship(
+        "BacktestStat", back_populates="trade_actions"
+    )
+
+
+# ---------------------------------------------------------------------------
+# unique_strategies  (read-only view / table)
+# ---------------------------------------------------------------------------
+
+
+class UniqueStrategy(Base):
+    __tablename__ = "unique_strategies"
+
+    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True)
+    ticker: Mapped[str | None] = mapped_column(Text, nullable=True)
+    strategy: Mapped[str | None] = mapped_column(Text, nullable=True)
+    period: Mapped[str | None] = mapped_column(Text, nullable=True)
+    interval: Mapped[str | None] = mapped_column(Text, nullable=True)
+    notifications_on: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
+    last_optimized_at: Mapped[datetime | None] = mapped_column(TIMESTAMPTZ, nullable=True)
+    tpsl_ratio: Mapped[float | None] = mapped_column(Double, nullable=True)
+    sl_coef: Mapped[float | None] = mapped_column(Double, nullable=True)
+    tp_coef: Mapped[float | None] = mapped_column(Double, nullable=True)
diff --git a/app/db/postgres.py b/app/db/postgres.py
new file mode 100644
index 0000000..1cf26d9
--- /dev/null
+++ b/app/db/postgres.py
@@ -0,0 +1,114 @@
+"""
+Async SQLAlchemy engine and session factory.
+
+DATABASE_URL is read from the environment. Accepted schemes (all normalised
+to postgresql+psycopg:// for psycopg3 async driver):
+    postgresql+psycopg://...    â† explicit, ideal
+    postgresql+asyncpg://...    â† auto-fixed (asyncpg has SCRAM bug with Supabase)
+    postgresql://...            â† auto-fixed
+    postgres://...              â† auto-fixed (common Supabase / Heroku form)
+    sqlite+aiosqlite://         â† used by unit tests
+
+The engine is created lazily on first use so this module can be imported
+during testing even when DATABASE_URL is not yet set.
+"""
+
+from __future__ import annotations
+
+import os
+from collections.abc import AsyncGenerator
+
+from dotenv import load_dotenv
+from sqlalchemy.ext.asyncio import (
+    AsyncSession,
+    async_sessionmaker,
+    create_async_engine,
+)
+from sqlalchemy.orm import DeclarativeBase
+
+load_dotenv()
+
+
+def _normalise_url(url: str) -> str:
+    """
+    Ensure the URL uses the psycopg3 async driver.
+    Rewrites any plain postgres:// or postgresql:// (which default to psycopg2)
+    and any postgresql+asyncpg:// (known broken with Supabase pgBouncer SCRAM).
+    """
+    for prefix in ("postgresql+asyncpg://", "postgresql+asyncio://", "asyncpg://"):
+        if url.startswith(prefix):
+            url = "postgresql+psycopg://" + url[len(prefix):]
+            return url
+    if url.startswith("postgres://"):
+        return url.replace("postgres://", "postgresql+psycopg://", 1)
+    if url.startswith("postgresql://"):
+        return url.replace("postgresql://", "postgresql+psycopg://", 1)
+    return url
+
+
+def _get_database_url() -> str:
+    raw = os.environ.get("DATABASE_URL", "")
+    if not raw:
+        raise RuntimeError(
+            "DATABASE_URL is not set. Add it to your .env file.\n"
+            "Format: postgresql+asyncpg://<user>:<pass>@<host>:<port>/<db>\n"
+            "Get it from: Supabase â†’ Settings â†’ Database â†’ Connection String â†’ URI"
+        )
+    return _normalise_url(raw)
+
+
+# ---------------------------------------------------------------------------
+# Lazy singletons â€” created on first access
+# ---------------------------------------------------------------------------
+
+_engine = None
+_factory: async_sessionmaker | None = None
+
+
+def _get_factory() -> async_sessionmaker:
+    global _engine, _factory
+    if _factory is None:
+        url = _get_database_url()
+        connect_args: dict = {}
+        if url.startswith("postgresql+psycopg://"):
+            # psycopg3 defaults to SCRAM-SHA-256-PLUS (TLS channel binding) when
+            # SSL is active. Supabase's pgBouncer does NOT support the -PLUS
+            # variant and rejects it as "Wrong password" even with correct creds.
+            # Disabling channel binding forces plain SCRAM-SHA-256, which works.
+            connect_args["channel_binding"] = "disable"
+            # prepare_threshold=0 disables server-side prepared statements,
+            # required for pgBouncer Transaction Pooler (port 6543) compatibility.
+            connect_args["prepare_threshold"] = 0
+            # Supabase requires SSL on all pooler connections.
+            if "supabase" in url or "pooler" in url:
+                connect_args["sslmode"] = "require"
+        _engine = create_async_engine(
+            url,
+            echo=False,
+            pool_pre_ping=True,
+            pool_recycle=1800,
+            connect_args=connect_args,
+        )
+        _factory = async_sessionmaker(_engine, expire_on_commit=False, class_=AsyncSession)
+    return _factory
+
+
+def AsyncSessionLocal() -> AsyncSession:  # noqa: N802
+    """
+    Return an async context-manager session.
+
+    Usage (in repositories / services):
+        async with AsyncSessionLocal() as session:
+            ...
+    """
+    return _get_factory()()
+
+
+class Base(DeclarativeBase):
+    """Shared declarative base for all SQLAlchemy ORM models."""
+
+
+async def get_db() -> AsyncGenerator[AsyncSession, None]:
+    """FastAPI dependency â€” yields one session per request."""
+    async with _get_factory()() as session:
+        yield session
diff --git a/app/db/repository.py b/app/db/repository.py
new file mode 100644
index 0000000..6e66580
--- /dev/null
+++ b/app/db/repository.py
@@ -0,0 +1,104 @@
+"""
+Repository layer for all Postgres ORM operations.
+
+Each repository receives an AsyncSession and exposes typed async methods.
+They replace all direct supabase.table(...) calls.
+"""
+
+from __future__ import annotations
+
+from typing import Any
+
+from sqlalchemy import select
+from sqlalchemy.dialects.postgresql import insert as pg_insert
+from sqlalchemy.ext.asyncio import AsyncSession
+
+from app.db.models import BacktestStat, TradeAction, UniqueStrategy
+
+# ---------------------------------------------------------------------------
+# BacktestStatRepository
+# ---------------------------------------------------------------------------
+
+
+class BacktestStatRepository:
+    def __init__(self, session: AsyncSession) -> None:
+        self._session = session
+
+    async def upsert(self, data: dict[str, Any]) -> BacktestStat | None:
+        """
+        Update an existing backtest stat matching the strategy config, or insert if none exists.
+        Postgres doesn't have a unique constraint on these 4 columns, so we manually query.
+        """
+        stmt = select(BacktestStat).where(
+            BacktestStat.ticker == data.get("ticker"),
+            BacktestStat.strategy == data.get("strategy"),
+            BacktestStat.period == data.get("period"),
+            BacktestStat.interval == data.get("interval"),
+        )
+        result = await self._session.execute(stmt)
+        existing = result.scalars().first()
+
+        if existing:
+            for k, v in data.items():
+                if k != "id":
+                    setattr(existing, k, v)
+            await self._session.commit()
+            return existing
+        else:
+            return await self.insert(data)
+
+    async def insert(self, data: dict[str, Any]) -> BacktestStat | None:
+        """INSERT a new row and return the created model."""
+        stmt = pg_insert(BacktestStat).values(**data).returning(BacktestStat)
+        result = await self._session.execute(stmt)
+        await self._session.commit()
+        return result.scalar_one_or_none()
+
+
+# ---------------------------------------------------------------------------
+# TradeActionRepository
+# ---------------------------------------------------------------------------
+
+
+class TradeActionRepository:
+    def __init__(self, session: AsyncSession) -> None:
+        self._session = session
+
+    async def get_latest_for_strategy(
+        self, backtest_id: int
+    ) -> TradeAction | None:
+        """Return the most recent TradeAction for a given backtest_id."""
+        stmt = (
+            select(TradeAction)
+            .where(TradeAction.backtest_id == backtest_id)
+            .order_by(TradeAction.datetime.desc())
+            .limit(1)
+        )
+        result = await self._session.execute(stmt)
+        return result.scalar_one_or_none()
+
+    async def insert_many(
+        self, records: list[dict[str, Any]]
+    ) -> list[TradeAction]:
+        """Bulk-insert trade action records and return the created models."""
+        if not records:
+            return []
+        stmt = pg_insert(TradeAction).values(records).returning(TradeAction)
+        result = await self._session.execute(stmt)
+        await self._session.commit()
+        return list(result.scalars().all())
+
+
+# ---------------------------------------------------------------------------
+# UniqueStrategyRepository
+# ---------------------------------------------------------------------------
+
+
+class UniqueStrategyRepository:
+    def __init__(self, session: AsyncSession) -> None:
+        self._session = session
+
+    async def get_all(self) -> list[UniqueStrategy]:
+        """Return all unique strategies."""
+        result = await self._session.execute(select(UniqueStrategy))
+        return list(result.scalars().all())
diff --git a/app/notification/service.py b/app/notification/service.py
index 371bb29..7c6aadb 100644
--- a/app/notification/service.py
+++ b/app/notification/service.py
@@ -73,14 +73,14 @@ def send_trade_action_notification(
     message_text = ""
     
     print(f"Sending trade action notification for {ticker} on {interval}...")
-    print(f"Trade actions: {trade_actions.data}")
+    print(f"Trade actions: {trade_actions}")
 
-    for action in trade_actions.data:
-        strategy_performance_url = f"{os.environ['OKANE_SIGNALS_URL']}/strategy/{action['backtest_id']}"
-        backtest_results_url = f"{os.environ['OKANE_SIGNALS_URL']}/strategy/{action['backtest_id']}/backtest"
+    for action in trade_actions:
+        strategy_performance_url = f"{os.environ['OKANE_SIGNALS_URL']}/strategy/{action.backtest_id}"
+        backtest_results_url = f"{os.environ['OKANE_SIGNALS_URL']}/strategy/{action.backtest_id}/backtest"
         
-        if action['trade_action'] == "buy":
-            content = f"ðŸŸ¢ BUY signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ° Interval: {interval} \nâ±ï¸Time: {action['datetime']} (GMT) \n\nEntry: {str(action['entry_price'])[0:7]} \nSize: {action['size']} \nStop loss: {str(action['sl'])[0:7]} \nTake Profit: {str(action['tp'])[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
+        if action.trade_action == "buy":
+            content = f"ðŸŸ¢ BUY signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ° Interval: {interval} \nâ±ï¸Time: {action.datetime} (GMT) \n\nEntry: {str(action.entry_price)[0:7]} \nSize: {action.size} \nStop loss: {str(action.sl)[0:7]} \nTake Profit: {str(action.tp)[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
             messages.append(
                 {
                     "type": "text",
@@ -88,8 +88,8 @@ def send_trade_action_notification(
                 }
             )
             message_text = message_text + content + "\n\n"
-        elif action['trade_action'] == "sell":
-            content = f"ðŸ”´ SELL signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ³ Interval: {interval} \nâ±ï¸Time: {action['datetime']} (GMT) \n\nEntry: {str(action['entry_price'])[0:7]} \nSize: {action['size']} \nStop loss: {str(action['sl'])[0:7]} \nTake Profit: {str(action['tp'])[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
+        elif action.trade_action == "sell":
+            content = f"ðŸ”´ SELL signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ³ Interval: {interval} \nâ±ï¸Time: {action.datetime} (GMT) \n\nEntry: {str(action.entry_price)[0:7]} \nSize: {action.size} \nStop loss: {str(action.sl)[0:7]} \nTake Profit: {str(action.tp)[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
             messages.append(
                 {
                     "type": "text",
@@ -97,8 +97,8 @@ def send_trade_action_notification(
                 }
             )
             message_text = message_text + content + "\n\n"
-        elif action['trade_action'] == "close":
-            content = f"ðŸŸ¡ CLOSE signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ³ Interval: {interval} \nâ±ï¸Time: {action['datetime']} (GMT) \n\nEntry: {str(action['entry_price'])[0:7]} \nSize: {action['size']} \nClose Price: {str(action['price'])[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
+        elif action.trade_action == "close":
+            content = f"ðŸŸ¡ CLOSE signal\n\nðŸ§  Strategy: {strategy} \nðŸ“ˆ Symbol: {ticker}\nâ³ Interval: {interval} \nâ±ï¸Time: {action.datetime} (GMT) \n\nEntry: {str(action.entry_price)[0:7]} \nSize: {action.size} \nClose Price: {str(action.price)[0:7]} \n\nStrategy: {strategy_performance_url} \nBacktest: {backtest_results_url}\n---\n"
             messages.append(
                 {
                     "type": "text",
diff --git a/app/signals/router.py b/app/signals/router.py
index 3751191..ece1fa5 100644
--- a/app/signals/router.py
+++ b/app/signals/router.py
@@ -1,19 +1,18 @@
-from fastapi import APIRouter, Depends, BackgroundTasks
+from typing import Annotated
+
+import uuid
+
+from fastapi import APIRouter, BackgroundTasks, Depends
+from starlette.status import HTTP_200_OK
+
+from app.auth.basic_auth import get_current_username
+from app.signals import service
 from app.signals.dto import (
+    BacktestProcessResponseDTO,
+    BacktestResponseDTO,
     SignalRequestDTO,
     SignalResponseDTO,
-    BacktestResponseDTO,
-    BacktestProcessResponseDTO,
 )
-from app.signals import service
-from starlette.status import HTTP_200_OK
-from app.auth.basic_auth import get_current_username
-import uuid
-import asyncio
-from concurrent.futures import ThreadPoolExecutor
-from typing import Annotated
-
-executor = ThreadPoolExecutor(max_workers=5)
 
 router = APIRouter(
     prefix="/signals",
@@ -22,14 +21,6 @@ router = APIRouter(
 )
 
 
-def run_in_executor(func, *args, **kwargs):
-    def wrapper():
-        return func(*args, **kwargs)
-
-    loop = asyncio.new_event_loop()
-    return asyncio.ensure_future(loop.run_in_executor(executor, wrapper))
-
-
 # Query parameters: ticker, start, end, interval, strategy, and parameters
 @router.get("/", response_model=SignalResponseDTO, status_code=HTTP_200_OK)
 async def get_signals(
@@ -57,22 +48,19 @@ async def backtest(
 ) -> BacktestProcessResponseDTO:
     backtest_process_uuid = uuid.uuid4()
 
-    # Save the UUID as a new entry in the trade actions database and return the UUID
-
-    def execute_backtest():
-        service.get_backtest_result(
-            ticker=params.ticker,
-            interval=params.interval,
-            period=params.period,
-            strategy=params.strategy,
-            parameters=params.parameters,
-            start=params.start,
-            end=params.end,
-            strategy_id=params.strategy_id,
-            backtest_process_uuid=params.backtest_process_uuid,
-        )
-
-    background_tasks.add_task(run_in_executor, execute_backtest)
+    # Schedule the async work as a FastAPI background task
+    background_tasks.add_task(
+        service.get_backtest_result,
+        ticker=params.ticker,
+        interval=params.interval,
+        period=params.period,
+        strategy=params.strategy,
+        parameters=params.parameters,
+        start=params.start,
+        end=params.end,
+        strategy_id=params.strategy_id,
+        backtest_process_uuid=params.backtest_process_uuid,
+    )
     return str(backtest_process_uuid)
 
 
@@ -81,28 +69,22 @@ async def backtest(
 )
 async def backtest_sync(
     username: Annotated[str, Depends(get_current_username)],
-    background_tasks: BackgroundTasks,
     params: SignalRequestDTO = Depends(),
 ) -> BacktestResponseDTO:
-
-    def execute_backtest():
-        service.get_backtest_result(
-            ticker=params.ticker,
-            interval=params.interval,
-            period=params.period,
-            strategy=params.strategy,
-            parameters=params.parameters,
-            start=params.start,
-            end=params.end,
-        )
-
-    return execute_backtest()
+    return await service.get_backtest_result(
+        ticker=params.ticker,
+        interval=params.interval,
+        period=params.period,
+        strategy=params.strategy,
+        parameters=params.parameters,
+        start=params.start,
+        end=params.end,
+    )
 
 
 @router.post("/strategy-notification-job", status_code=HTTP_200_OK)
 async def strategy_notification(
     username: Annotated[str, Depends(get_current_username)],
 ) -> None:
-    service.strategy_notification_job()
-
+    await service.strategy_notification_job()
     return None
diff --git a/app/signals/service.py b/app/signals/service.py
index 257c6bb..d6008f6 100644
--- a/app/signals/service.py
+++ b/app/signals/service.py
@@ -1,82 +1,46 @@
+import asyncio
+import base64
+import json
+import logging
+import os
 import zlib
-import yfinance as yf
-from fastapi import APIRouter, Depends, BackgroundTasks
-from fastapi import FastAPI, HTTPException
-from app.base.utils.mongodb import connect_mongodb, COLLECTIONS
+from concurrent.futures import ThreadPoolExecutor
+from datetime import UTC, datetime
+
+from fastapi import HTTPException
 from starlette.status import HTTP_200_OK
-from app.lib.utils.pako import pako_deflate
-from app.signals.utils.yfinance import getYFinanceData, getYFinanceDataAsync
-from app.signals.utils.signals import get_latest_signal, get_all_signals
-from app.signals.strategies.calculate import calculate_signals, calculate_signals_async
 
-# from app.signals.strategies.ema_bollinger_backtest import backtest
-from app.signals.strategies.perform_backtest import perform_backtest
-from utils.supabase_client import supabase
+from app.db.postgres import AsyncSessionLocal
+from app.db.repository import (
+    BacktestStatRepository,
+    TradeActionRepository,
+    UniqueStrategyRepository,
+)
 from app.notification.service import send_trade_action_notification
-from app.signals.dto import SignalRequestDTO
-from concurrent.futures import ThreadPoolExecutor
-from typing import List
-import json
-import os
-import logging
-import asyncio
-from datetime import datetime, timezone, timedelta
-import urllib.parse
-import base64
+from app.signals.strategies.calculate import calculate_signals, calculate_signals_async
+from app.signals.strategies.perform_backtest import perform_backtest
+from app.signals.utils.signals import get_all_signals, get_latest_signal
+from app.signals.utils.yfinance import getYFinanceData, getYFinanceDataAsync
 
 executor = ThreadPoolExecutor(max_workers=5)
 
 
-def run_in_executor(func, *args, **kwargs):
-    def wrapper():
-        return func(*args, **kwargs)
-
-    loop = asyncio.new_event_loop()
-    return asyncio.ensure_future(loop.run_in_executor(executor, wrapper))
-
-
 async def get_signals(
     ticker, interval, period, strategy, parameters, start=None, end=None
 ):
-    """
-    Retrieves signals for a given ticker using the specified parameters.
-
-    Args:
-        ticker (str): The ticker symbol of the stock.
-        interval (str): The time interval for the stock data (e.g., '1d' for daily).
-        period (str): The time period for the stock data (e.g., '1y' for 1 year).
-        strategy (str): The strategy to use for signal calculation.
-        parameters (dict): The parameters required for the specified strategy.
-        start (str, optional): The start date for the stock data. Defaults to None.
-        end (str, optional): The end date for the stock data. Defaults to None.
-
-    Returns:
-        dict: A dictionary containing the status, message, and data of the signals.
-
-    Raises:
-        Exception: If there is an error fetching data from Yahoo Finance or calculating signals.
-    """
-    
+    """Retrieves signals for a given ticker using the specified parameters."""
     try:
-        # Fetch data from Yahoo Finance
         df = None
         df1d = None
         try:
             df = await getYFinanceDataAsync(ticker, interval, period, start, end)
-            
-            # Special adjustments for different strategies
-            if (strategy == "macd_1"):
+            if strategy == "macd_1":
                 df1d = getYFinanceData(ticker, "1d", period, start, end)
-                
-            # ...
-            
-            
         except Exception as e:
             raise HTTPException(
                 status_code=400, detail=f"Failed to calculate signals. Error: {e}"
             )
 
-        # Calculate signals
         signals_df = None
         try:
             signals_df = await calculate_signals_async(df, df1d, strategy, parameters)
@@ -87,10 +51,7 @@ async def get_signals(
         current_signal = signals_df.iloc[-1]
         current_signal = json.loads(current_signal.to_json())
 
-        # Get the latest signal
         latest_signal = get_latest_signal(signals_df)
-
-        # All Signals
         all_signals = get_all_signals(signals_df)
 
         return {
@@ -109,10 +70,9 @@ async def get_signals(
         }
     except Exception as e:
         raise HTTPException(status_code=400, detail=f"Failed to get signals. Error: {e}")
-        
 
 
-def get_backtest_result(
+async def get_backtest_result(
     ticker,
     interval,
     period,
@@ -129,23 +89,9 @@ def get_backtest_result(
     """
     Get the backtest result for a given ticker, interval, period, strategy, and parameters.
 
-    Args:
-        ticker (str): The ticker symbol of the asset.
-        interval (str): The time interval for the data (e.g., '1d' for daily, '1h' for hourly).
-        period (str): The period of data to fetch (e.g., '1y' for 1 year, '3mo' for 3 months).
-        strategy (str): The name of the strategy to use for backtesting.
-        parameters (dict): The parameters specific to the strategy.
-        start (str, optional): The start date of the backtest period (YYYY-MM-DD). Defaults to None.
-        end (str, optional): The end date of the backtest period (YYYY-MM-DD). Defaults to None.
-        strategy_id (str, optional): The ID of the strategy. Defaults to None.
-        backtest_process_uuid (str, optional): The UUID of the backtest process. Defaults to None.
-
-    Returns:
-        dict: A dictionary containing the backtest results.
-
-    Raises:
-        Exception: If there is an error fetching data from Yahoo Finance or calculating signals.
-
+    The heavy CPU work (data fetch + backtest computation) runs in a thread pool
+    via asyncio.to_thread() so it doesn't block the event loop.
+    DB writes are done directly in the async portion.
     """
     print(f"\n--- BACKTEST BEGINS --\n--- Start backtest for {ticker} ---\n")
     print(f"Interval: {interval}")
@@ -156,253 +102,105 @@ def get_backtest_result(
         print(f"Start Date: {start}")
     if end:
         print(f"End Date: {end}")
-        
-    
-    df = None
-    df1d = None
-    try:
-        df = getYFinanceData(ticker, interval, period, start, end)
-        if (strategy == "macd_1"):
-            df1d = getYFinanceData(ticker, "1d", period, start, end)
-    except Exception as e:
-        print(e)
-        raise HTTPException(
-            status_code=400, detail=f"Failed to calculate signals. Error: {e}"
-        )
 
     try:
-        print("Parameters: ", parameters)
-        if parameters is not None:
-            parameters_dict = json.loads(parameters)
+        parameters_dict = json.loads(parameters) if parameters is not None else {}
     except Exception as e:
-        print(e)
         raise HTTPException(
             status_code=400, detail=f"Failed to parse parameters. Error: {e}"
         )
 
-    print(f"\n--- Calculating signals ---\n")
-    signals_df = None
-    try:
+    # -----------------------------------------------------------------
+    # Run CPU-bound work in a thread (doesn't block the event loop)
+    # -----------------------------------------------------------------
+    def _run_backtest():
+        """Sync work: fetch data + compute backtest. Returns (bt, stats, trade_actions, strategy_parameters)."""
+        df = getYFinanceData(ticker, interval, period, start, end)
+        df1d = getYFinanceData(ticker, "1d", period, start, end) if strategy == "macd_1" else None
+
         signals_df = calculate_signals(df, df1d, strategy, parameters_dict)
-    except Exception as e:
-        print("calculate_signals: ERROR: ", e)
-        raise HTTPException(
-            status_code=400, detail=f"Failed to calculate signals. Error: {e}"
-        )
-        
-    size = 0.03
-    if (ticker == "BTC-USD"):
-        size = 0.01  
-
-    bt, stats, trade_actions, strategy_parameters = perform_backtest(
-        signals_df,
-        strategy,
-        {
-            "best": False,
-            "size": size,
-            "slcoef": 2.2,
-            "tpslRatio": 2.0,
-            "max_longs": parameters_dict.get("max_longs", 1),
-            "max_shorts": parameters_dict.get("max_shorts", 1),
-        },
-        skip_optimization,
-        best_params,
-    )
-    
-    # # Print resulting trade actions
-    # print("\n--- Trade Actions ---\n")
-    # print(trade_actions)
-
-    # if bt is None or stats is None:
-    #     raise HTTPException(
-    #         status_code=400, detail=f"Failed to perform backtest for strategy: {strategy}"
-    #     )
-
-    print(f"\n--- Creating backtest result HTML ---\n")
-    bt.plot(open_browser=False, filename="backtest.html")
-    # Read the HTML content
-    with open("backtest.html", "r") as file:
-        html_content = file.read()
-        # delete the html file
-        file.close()
-        os.remove("backtest.html")
-    logging.info("get_backtest_result finished")
 
-    print("Original trade actions")
-    print(len(trade_actions))
+        size = 0.01 if ticker == "BTC-USD" else 0.03
+
+        return perform_backtest(
+            signals_df,
+            strategy,
+            {
+                "best": False,
+                "size": size,
+                "slcoef": 2.2,
+                "tpslRatio": 2.0,
+                "max_longs": parameters_dict.get("max_longs", 1),
+                "max_shorts": parameters_dict.get("max_shorts", 1),
+            },
+            skip_optimization,
+            best_params,
+        )
 
-    # add the backtest_id
-    for trade_action in trade_actions:
-        trade_action["backtest_id"] = strategy_id
-        
-    print(f"\n--- Saving Trade Actions ---\n")
     try:
-        latest_trade_action = None
-        if strategy_id != None:
-            latest_trade_action = (
-                supabase.table("trade_actions")
-                .select("*")
-                .eq("backtest_id", strategy_id)
-                .order("datetime", desc=True)
-                .limit(1)
-                .execute()
+        bt, stats, trade_actions, strategy_parameters = await asyncio.to_thread(_run_backtest)
+        if bt is None or stats is None:
+            raise HTTPException(
+                status_code=400, detail="Backtest returned no results (strategy calculation may have failed or no trades were executed)."
             )
-            print("----Latest Trade Action----")
-            print(latest_trade_action)
-
-            print("----New Trade Actions----")
-            if len(latest_trade_action.data) > 0:
-                # filter only records where the datetime is newer than latest_trade_action.data[0]['datetime']
-                trade_actions = [
-                    trade_action
-                    for trade_action in trade_actions
-                    if trade_action["datetime"]
-                    > latest_trade_action.data[0]["datetime"]
-                ]
-            else:
-                trade_actions = trade_actions[-1:]
-        else:
-            trade_actions = trade_actions[-1:]
-
-        print(len(trade_actions))
     except Exception as e:
-        logging.error(
-            f"Failed to get the latest trade action from the database. Error: {e}"
+        if isinstance(e, HTTPException):
+            raise e
+        raise HTTPException(
+            status_code=400, detail=f"Failed to run backtest. Error: {e}"
         )
 
-    ### Saving data to the database ###
-    print ("\n--- Saving data to DB ---\n")
-    # Save backtest stats to the database
-    backtest_stats = {
-        "ticker": ticker,
-        "max_drawdown_percentage": round(float(stats["Max. Drawdown [%]"]), 3),
-        "start_time": stats["Start"].strftime("%Y-%m-%d %H:%M:%S.%f"),
-        "end_time": stats["End"].strftime("%Y-%m-%d %H:%M:%S.%f"),
-        "duration": str(stats["Duration"]),
-        "exposure_time_percentage": round(float(stats["Exposure Time [%]"]), 3),
-        "final_equity": round(float(stats["Equity Final [$]"]), 3),
-        "peak_equity": round(float(stats["Equity Peak [$]"]), 3),
-        "return_percentage": round(float(stats["Return [%]"]), 3),
-        "buy_and_hold_return": round(float(stats["Buy & Hold Return [%]"]), 3),
-        "return_annualized": round(float(stats["Return (Ann.) [%]"]), 3),
-        "volatility_annualized": round(float(stats["Volatility (Ann.) [%]"]), 3),
-        "sharpe_ratio": round(float(stats["Sharpe Ratio"]), 3),
-        "sortino_ratio": round(float(stats["Sortino Ratio"]), 3),
-        "calmar_ratio": round(float(stats["Calmar Ratio"]), 3),
-        "max_drawdown_percentage": round(float(stats["Max. Drawdown [%]"]), 3),
-        "average_drawdown_percentage": round(float(stats["Avg. Drawdown [%]"]), 3),
-        "max_drawdown_duration": str(stats["Max. Drawdown Duration"]),
-        "average_drawdown_duration": str(stats["Avg. Drawdown Duration"]),
-        "trade_count": stats["# Trades"],
-        "win_rate": round(float(stats["Win Rate [%]"]), 3),
-        "best_trade": round(float(stats["Best Trade [%]"]), 3),
-        "worst_trade": round(float(stats["Worst Trade [%]"]), 3),
-        "avg_trade": round(float(stats["Avg. Trade [%]"]), 3),
-        "max_trade_duration": str(stats["Max. Trade Duration"]),
-        "average_trade_duration": str(stats["Avg. Trade Duration"]),
-        "profit_factor": round(float(stats["Profit Factor"]), 3),
-        "html": html_content,
-        "strategy": strategy,
-        "period": period,
-        "interval": interval,
-        "ref_id": backtest_process_uuid,
-        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
-        "last_optimized_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
-        "tpsl_ratio": round(float(strategy_parameters.get("tpslRatio", None)), 3) if strategy_parameters.get("tpslRatio") not in [None, ""] else None,
-        "sl_coef": round(float(strategy_parameters.get("slcoef", None)), 3) if strategy_parameters.get("slcoef") not in [None, ""] else None,
-        "tp_coef": round(float(strategy_parameters.get("TPcoef", None)), 3) if strategy_parameters.get("TPcoef") not in [None, ""] else None,
-    }
-    
-    # Defalt the HTML content to save bandwidth and storage
-    try:
-        # Convert to bytes
-        html_bytes = html_content.encode('utf-8')
-        # Compress with zlib (default DEFLATE format)
-        compressed_data = zlib.compress(html_bytes, level=9)
-        # Encode in base64 for transmission
-        backtest_stats["html"] = base64.b64encode(compressed_data).decode('utf-8')
-    except Exception as e: 
-        logging.error("Failed to deflate the HTML content", e)
-        
-    # Enable notification if sharpe_ratio is positive and return_percentage is positive
-    if (
-        (backtest_stats["sharpe_ratio"] > 0 and backtest_stats["return_percentage"] > 0) or
-        (backtest_stats["win_rate"] > 60)
-    ):
-        backtest_stats["notifications_on"] = True
-        notifications_on = True
-    else: 
-        backtest_stats["notifications_on"] = False
-        notifications_on = False
-    
-    if backtest_stats["sharpe_ratio"] > 0 and backtest_stats["return_percentage"] > 0:
-        backtest_stats["notifications_on"] = True
-        notifications_on = True
-    
-    if strategy_id != None:
-        backtest_stats["id"] = strategy_id
-
-    updated_backtest_stats = None
-    try:
-        logging.info(f"Saving backtest stats to the database. Ticker: {ticker}")
-        if strategy_id != None:
-            updated_backtest_stats = (
-                supabase.table("backtest_stats").upsert(backtest_stats, returning='minimal').execute()
-            )
-        else:
-            updated_backtest_stats = (
-                supabase.table("backtest_stats").insert([backtest_stats], returning='minimal').execute()
-            )
-    except Exception as e:
-        logging.error(f"Failed to save backtest stats to the database. Error: {e}")
+    # Generate the HTML plot (also sync/CPU-bound)
+    def _render_html():
+        bt.plot(open_browser=False, filename="backtest.html")
+        with open("backtest.html") as f:
+            content = f.read()
+        os.remove("backtest.html")
+        return content
 
-    # Add backtest_id to trade_actions
-    # add the backtest_id
-    try:
-        if updated_backtest_stats and updated_backtest_stats.data:
-            for trade_action in trade_actions:
-                print("\n\n")
-                print(trade_action, "\n\n")
-                # Add the backtest_stat id (foreign key) to the trade action
-                if updated_backtest_stats.data[0]["id"] is not None:
-                    trade_action["backtest_id"] = updated_backtest_stats.data[0]["id"]
-        else:
-            logging.error("updated_backtest_stats or updated_backtest_stats.data is empty")
+    html_content = await asyncio.to_thread(_render_html)
+    logging.info("get_backtest_result finished")
 
-    except Exception as e:
-        logging.error(f"Failed to add backtest_id to trade_actions. Error: {e}")
+    print("Original trade actions:", len(trade_actions))
+
+    for ta in trade_actions:
+        ta["backtest_id"] = strategy_id
+
+    # -----------------------------------------------------------------
+    # Persist results to DB (async â€” runs on the event loop directly)
+    # -----------------------------------------------------------------
+    result = await _persist_backtest_result(
+        ticker=ticker,
+        interval=interval,
+        period=period,
+        strategy=strategy,
+        strategy_id=strategy_id,
+        backtest_process_uuid=backtest_process_uuid,
+        stats=stats,
+        html_content=html_content,
+        strategy_parameters=strategy_parameters,
+        trade_actions=trade_actions,
+        notifications_on=notifications_on,
+    )
 
-    # Save trade actions to the database
-    print(f"\n--- Saving trade actions to DB ---\n")
-    try:
-        logging.info(f"Saving trade actions to the database. Ticker: {ticker}")
-        print("--- Inserting Trade Actions to DB")
-        print(trade_actions)
-        if (len(trade_actions) > 0):
-            trade_actions = supabase.table("trade_actions").insert(trade_actions).execute()
-            print(f"Trade actions saved to the database.")
-    except Exception as e:
-        logging.error(f"Failed to save trade actions to the database. Error: {e}")
+    notifications_on = result["notifications_on"]
+    saved_trade_actions = result["trade_actions"]
 
-    # Send notifications for new trade actions
-    print(f"\n--- Sending trade action notifications ---\n")
-    if notifications_on & hasattr(trade_actions, 'data'):
+    print("\n--- Sending trade action notifications ---\n")
+    if notifications_on and saved_trade_actions:
         print("Sending trade action notification to LINE group...")
-        print(trade_actions)
         try:
             send_trade_action_notification(
                 strategy=strategy,
                 ticker=ticker,
                 interval=interval,
-                trade_actions=trade_actions,
+                trade_actions=saved_trade_actions,
             )
         except Exception as e:
             logging.error(f"Failed to send LINE notification. Error: {e}")
 
-    print(f"\n--- COMPLETE ---\n")
-    
-    # Return the response. No longer need to send the HTML payload since it will be saved to DB.
-    # Just return the id of those records
+    print("\n--- COMPLETE ---\n")
+
     return {
         "status": HTTP_200_OK,
         "message": "Backtest results",
@@ -410,60 +208,220 @@ def get_backtest_result(
     }
 
 
-def strategy_notification_job():
-    """
-    Send a notification for the specified strategies.
+async def _persist_backtest_result(
+    *,
+    ticker: str,
+    interval: str,
+    period: str,
+    strategy: str,
+    strategy_id,
+    backtest_process_uuid,
+    stats,
+    html_content: str,
+    strategy_parameters: dict,
+    trade_actions: list[dict],
+    notifications_on: bool,
+) -> dict:
+    """Async DB writes for a completed backtest."""
+    async with AsyncSessionLocal() as session:
+        backtest_repo = BacktestStatRepository(session)
+        trade_repo = TradeActionRepository(session)
+
+        # Build backtest_stats payload
+        backtest_stats_data: dict = {
+            "ticker": ticker,
+            "max_drawdown_percentage": round(float(stats["Max. Drawdown [%]"]), 3),
+            "start_time": stats["Start"].strftime("%Y-%m-%d %H:%M:%S.%f"),
+            "end_time": stats["End"].strftime("%Y-%m-%d %H:%M:%S.%f"),
+            "duration": str(stats["Duration"]),
+            "exposure_time_percentage": round(float(stats["Exposure Time [%]"]), 3),
+            "final_equity": round(float(stats["Equity Final [$]"]), 3),
+            "peak_equity": round(float(stats["Equity Peak [$]"]), 3),
+            "return_percentage": round(float(stats["Return [%]"]), 3),
+            "buy_and_hold_return": round(float(stats["Buy & Hold Return [%]"]), 3),
+            "return_annualized": round(float(stats["Return (Ann.) [%]"]), 3),
+            "volatility_annualized": round(float(stats["Volatility (Ann.) [%]"]), 3),
+            "sharpe_ratio": round(float(stats["Sharpe Ratio"]), 3),
+            "sortino_ratio": round(float(stats["Sortino Ratio"]), 3),
+            "calmar_ratio": round(float(stats["Calmar Ratio"]), 3),
+            "average_drawdown_percentage": round(float(stats["Avg. Drawdown [%]"]), 3),
+            "max_drawdown_duration": str(stats["Max. Drawdown Duration"]),
+            "average_drawdown_duration": str(stats["Avg. Drawdown Duration"]),
+            "trade_count": stats["# Trades"],
+            "win_rate": round(float(stats["Win Rate [%]"]), 3),
+            "best_trade": round(float(stats["Best Trade [%]"]), 3),
+            "worst_trade": round(float(stats["Worst Trade [%]"]), 3),
+            "avg_trade": round(float(stats["Avg. Trade [%]"]), 3),
+            "max_trade_duration": str(stats["Max. Trade Duration"]),
+            "average_trade_duration": str(stats["Avg. Trade Duration"]),
+            "profit_factor": round(float(stats["Profit Factor"]), 3),
+            "html": html_content,
+            "strategy": strategy,
+            "period": period,
+            "interval": interval,
+            "ref_id": backtest_process_uuid,
+            "updated_at": datetime.now(UTC),
+            "last_optimized_at": datetime.now(UTC),
+            "tpsl_ratio": (
+                round(float(strategy_parameters.get("tpslRatio")), 3)
+                if strategy_parameters.get("tpslRatio") not in [None, ""]
+                else None
+            ),
+            "sl_coef": (
+                round(float(strategy_parameters.get("slcoef")), 3)
+                if strategy_parameters.get("slcoef") not in [None, ""]
+                else None
+            ),
+            "tp_coef": (
+                round(float(strategy_parameters.get("TPcoef")), 3)
+                if strategy_parameters.get("TPcoef") not in [None, ""]
+                else None
+            ),
+        }
 
-    Args:
-        strategy_id_list (List[SignalRequestDTO]): A list of strategy IDs for which to send notifications.
+        # Deflate HTML
+        try:
+            compressed = zlib.compress(html_content.encode("utf-8"), level=9)
+            backtest_stats_data["html"] = base64.b64encode(compressed).decode("utf-8")
+        except Exception as e:
+            logging.error("Failed to deflate HTML: %s", e)
+
+        # Compute the metrics-based flag and persist it to the DB record.
+        # NOTE: this does NOT override the `notifications_on` parameter that was
+        # passed into this function â€” that value (from strategy.notifications_on)
+        # is the authoritative source for whether to actually send a notification.
+        good_sharpe = backtest_stats_data["sharpe_ratio"] > 0
+        good_return = backtest_stats_data["return_percentage"] > 0
+        good_winrate = backtest_stats_data["win_rate"] > 60
+        computed_notifications_on = (good_sharpe and good_return) or good_winrate
+        backtest_stats_data["notifications_on"] = computed_notifications_on
+        print(
+            f"Notifications â€” strategy flag: {notifications_on}, "
+            f"computed flag: {computed_notifications_on} "
+            f"(sharpe>0={good_sharpe}, return>0={good_return}, winrate>60={good_winrate})"
+        )
+
+        # Upsert backtest_stats
+        updated_stat = None
+        try:
+            logging.info("Saving backtest stats to DB. Ticker: %s", ticker)
+            updated_stat = await backtest_repo.upsert(backtest_stats_data)
+        except Exception as e:
+            logging.error("Failed to save backtest stats: %s", e)
+
+        # Determine which trade actions are "new" by querying against the correct backtest_stat ID
+        if updated_stat is not None:
+            print(f"[dedup] Querying latest trade action for backtest_stat.id={updated_stat.id}")
+            latest_ta = await trade_repo.get_latest_for_strategy(updated_stat.id)
+            print(f"[dedup] Total backtest trade actions before filter: {len(trade_actions)}")
+
+            if latest_ta is not None:
+                # Strip tzinfo from both sides so naive/aware mismatches never raise TypeError.
+                # psycopg3 can return TIMESTAMP WITHOUT TIME ZONE as tz-aware in some configs.
+                cutoff_dt = latest_ta.datetime
+                if hasattr(cutoff_dt, "tzinfo") and cutoff_dt.tzinfo is not None:
+                    cutoff_dt = cutoff_dt.replace(tzinfo=None)
+                print(f"[dedup] Cutoff datetime (latest DB trade action): {cutoff_dt!r}")
+
+                new_trade_actions = []
+                for ta in trade_actions:
+                    raw_dt = ta.get("datetime", "")
+                    try:
+                        ta_dt = datetime.strptime(raw_dt, "%Y-%m-%d %H:%M:%S.%f")
+                    except (ValueError, TypeError):
+                        try:
+                            ta_dt = datetime.strptime(raw_dt, "%Y-%m-%d %H:%M:%S")
+                        except (ValueError, TypeError):
+                            logging.warning(
+                                "[dedup] Could not parse trade action datetime: %s â€” skipping", raw_dt
+                            )
+                            continue
+                    passed = ta_dt > cutoff_dt
+                    print(f"[dedup]   ta_dt={ta_dt!r} > cutoff={cutoff_dt!r} â†’ {passed}")
+                    if passed:
+                        new_trade_actions.append(ta)
+
+                print(f"[dedup] Actions passing filter: {len(new_trade_actions)}")
+                trade_actions = new_trade_actions
+            else:
+                print("[dedup] No existing trade actions in DB for this strategy â€” using last backtest action only")
+                trade_actions = trade_actions[-1:]
 
-    Returns:
-        None
+            for ta in trade_actions:
+                ta["backtest_id"] = updated_stat.id
 
-    -- Supabase AI is experimental and may produce incorrect answers
-    -- Always verify the output before executing
+        else:
+            logging.error("updated_stat is empty â€” backtest_id not set on trade actions")
+            trade_actions = trade_actions[-1:]
 
+        # Insert trade actions
+        saved_trade_actions = []
+        try:
+            logging.info("Saving trade actions to DB. Ticker: %s", ticker)
+            if trade_actions:
+                saved_trade_actions = await trade_repo.insert_many(trade_actions)
+                print("Trade actions saved to DB.")
+        except Exception as e:
+            logging.error("Failed to save trade actions: %s", e)
+
+    return {
+        "notifications_on": notifications_on,
+        "trade_actions": saved_trade_actions,
+    }
+
+
+async def strategy_notification_job():
+    """
+    Fetch all unique strategies and run backtests + send notifications.
+    Now fully async â€” safe to call from an async FastAPI endpoint.
     """
-    # Perform the inner query
-    query = supabase.table("unique_strategies").select("*")
-    response = query.execute()
+    strategies = await _get_all_strategies()
 
     print("--------------------------------------")
-    print("Preparing to run backtests and send signal notification if available. \nSignal for: ", response.data)
+    print("Preparing to run backtests. Signal for:", strategies)
     print("--------------------------------------")
-    logging.info(response.data)
+    logging.info(strategies)
 
-    for strategy in response.data:
+    for strategy in strategies:
         logging.info(
-            f"Updating strategy backtest. Ticker: {strategy['ticker']}, Strategy: {strategy['strategy']}, Period: {strategy['period']}, Interval: {strategy['interval']}"
+            "Updating strategy backtest. Ticker: %s, Strategy: %s, Period: %s, Interval: %s",
+            strategy.ticker, strategy.strategy, strategy.period, strategy.interval,
         )
-        
-        # Check the strategy["last_optimized_at"] and compare it with the current time. If the difference is greater than 5 days, re-optimize.
-        singapore_tz = timezone(timedelta(hours=8))
-        last_optimized_at = datetime.strptime(strategy["last_optimized_at"], "%Y-%m-%dT%H:%M:%S.%f%z")
-        current_time = datetime.now(singapore_tz)
-        time_difference = (current_time - last_optimized_at).days
-        print("Skip optimization: ", time_difference < 3)
-
-        # Send a notification for the strategy
+
+        # last_optimized_at is returned as a native datetime by psycopg3
+        last_optimized_at = strategy.last_optimized_at
+        if last_optimized_at is None:
+            time_difference = 999  # treat as never optimized â†’ always optimize
+        else:
+            # Ensure tz-aware for comparison
+            if last_optimized_at.tzinfo is None:
+                last_optimized_at = last_optimized_at.replace(tzinfo=UTC)
+            time_difference = (datetime.now(UTC) - last_optimized_at).days
+        print("Skip optimization:", time_difference < 3)
+
         try:
-            get_backtest_result(
-                ticker=strategy["ticker"],
-                interval=strategy["interval"],
-                period=strategy["period"],
-                strategy=strategy["strategy"],
-                parameters='{"max_longs": 2, "max_shorts": 2}', # temporary
+            await get_backtest_result(
+                ticker=strategy.ticker,
+                interval=strategy.interval,
+                period=strategy.period,
+                strategy=strategy.strategy,
+                parameters='{"max_longs": 2, "max_shorts": 2}',
                 start=None,
                 end=None,
-                strategy_id=strategy["id"],
-                notifications_on=strategy["notifications_on"],
-                skip_optimization=time_difference < 3, # Reoptimize every 3 days
+                strategy_id=str(strategy.id) if strategy.id else None,
+                notifications_on=strategy.notifications_on,
+                skip_optimization=time_difference < 3,
                 best_params={
-                    "tpslRatio": strategy.get('tpsl_ratio'),
-                    "slcoef": strategy.get('sl_coef'),
-                    "TPcoef": strategy.get('tp_coef'),
-                    "grid_distance": strategy.get('grid_distance')
+                    "tpslRatio": strategy.tpsl_ratio,
+                    "slcoef": strategy.sl_coef,
+                    "TPcoef": strategy.tp_coef,
                 },
             )
         except Exception as e:
-            logging.error(f"Failed to send notification for strategy. Error: {e}")
+            logging.error("Failed to run backtest for strategy: %s", e)
+
+
+async def _get_all_strategies():
+    """Fetch all unique strategies from Postgres via ORM."""
+    async with AsyncSessionLocal() as session:
+        return await UniqueStrategyRepository(session).get_all()
diff --git a/app/signals/strategies/forex_fvg_respected/fvg_confirmation_backtest.py b/app/signals/strategies/forex_fvg_respected/fvg_confirmation_backtest.py
index 7dd563d..c8103fe 100644
--- a/app/signals/strategies/forex_fvg_respected/fvg_confirmation_backtest.py
+++ b/app/signals/strategies/forex_fvg_respected/fvg_confirmation_backtest.py
@@ -113,7 +113,7 @@ def backtest(df, strategy_parameters, size=0.03, skip_optimization=False, best_p
     margin = 1/500
     cash = 100000
 
-    QuantFVGStrategy.tp_sl_ratio = strategy_parameters.get('tp_sl_ratio', QuantFVGStrategy.tp_sl_ratio)
+    QuantFVGStrategy.tp_sl_ratio = strategy_parameters.get('tpslRatio', QuantFVGStrategy.tp_sl_ratio)
     QuantFVGStrategy.fvg_min_size_atr_multiplier = strategy_parameters.get('fvg_min_size_atr_multiplier', QuantFVGStrategy.fvg_min_size_atr_multiplier)
     QuantFVGStrategy.fvg_candle_range_atr_multiplier = strategy_parameters.get('fvg_candle_range_atr_multiplier', QuantFVGStrategy.fvg_candle_range_atr_multiplier)
     QuantFVGStrategy.sl_atr_multiplier = strategy_parameters.get('sl_atr_multiplier', QuantFVGStrategy.sl_atr_multiplier)
@@ -132,11 +132,11 @@ def backtest(df, strategy_parameters, size=0.03, skip_optimization=False, best_p
         
         if stats is not None and '_strategy' in stats:
             best_params = {
-                'tp_sl_ratio': stats['_strategy'].tp_sl_ratio
+                'tpslRatio': stats['_strategy'].tp_sl_ratio
             }
         else:
             best_params = {
-                'tp_sl_ratio': QuantFVGStrategy.tp_sl_ratio
+                'tpslRatio': QuantFVGStrategy.tp_sl_ratio
             }
         print("Best params from optimization:", best_params)
 
@@ -144,10 +144,11 @@ def backtest(df, strategy_parameters, size=0.03, skip_optimization=False, best_p
         print("Skipping optimization for FVG Confirmation Strategy.")
         if not best_params:
             best_params = {
-                'tp_sl_ratio': strategy_parameters.get('tp_sl_ratio', QuantFVGStrategy.tp_sl_ratio)
+                'tpslRatio': strategy_parameters.get('tpslRatio', QuantFVGStrategy.tp_sl_ratio)
             }
     
-    QuantFVGStrategy.tp_sl_ratio = best_params['tp_sl_ratio']
+    # Safely extract tp_sl_ratio; tolerate both keys just in case
+    QuantFVGStrategy.tp_sl_ratio = best_params.get('tpslRatio', best_params.get('tp_sl_ratio', QuantFVGStrategy.tp_sl_ratio))
     QuantFVGStrategy.log_trades = True
     QuantFVGStrategy.mysize = size
     
@@ -160,7 +161,7 @@ def backtest(df, strategy_parameters, size=0.03, skip_optimization=False, best_p
 
     strategy_parameters = {
         "best": True,
-        "tpslRatio": best_params['tp_sl_ratio']
+        "tpslRatio": QuantFVGStrategy.tp_sl_ratio
     }
     
     print("Final strategy parameters used:", strategy_parameters)
diff --git a/pyproject.toml b/pyproject.toml
index 7a75941..f81423c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -65,13 +65,10 @@ dependencies = [
     "sqlalchemy>=2.0.38",
     "dnspython>=2.7.0",
 
-    # â”€â”€ Supabase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-    "supabase>=2.13.0",
-    "gotrue>=2.11.0",
-    "storage3>=0.11.0",
-    "postgrest>=0.19.0",
-    "supafunc>=0.9.0",
-    "realtime>=2.3.0",
+    # â”€â”€ PostgreSQL async driver (SQLAlchemy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+    # psycopg3 (async) â€” better Supabase/pgBouncer compatibility than asyncpg
+    "psycopg[async,binary]>=3.2.0",
+    "greenlet>=3.0.0",
 
     # â”€â”€ HTTP / Async â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     "httpx>=0.28.0",
@@ -197,6 +194,10 @@ dev-dependencies = [
     "ruff>=0.8.0",
     "mypy>=1.13.0",
     "ipython>=8.18.0",
+    # â”€â”€ Test DB helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+    "aiosqlite>=0.20.0",
+    "mongomock-motor>=0.0.32",
+    "httpx>=0.28.0",
 ]
 
 [tool.uv.sources]
diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
index 0000000..a30ae9e
--- /dev/null
+++ b/tests/__init__.py
@@ -0,0 +1 @@
+# tests/
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000..928bf48
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,78 @@
+"""
+pytest configuration and shared fixtures for the test suite.
+
+Unit tests use:
+  - SQLite in-memory (via aiosqlite) for Postgres repository tests
+    â†’ No live Postgres connection required
+  - mongomock-motor for MongoDB tests
+    â†’ No live MongoDB connection required
+
+Integration tests (marked @pytest.mark.integration) are skipped unless
+the DATABASE_URL env var points at a real Postgres instance.
+"""
+
+from __future__ import annotations
+
+import os
+
+import pytest
+import pytest_asyncio
+from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
+
+from app.db.models import Base
+
+
+# ---------------------------------------------------------------------------
+# SQLite in-memory engine for unit tests
+# ---------------------------------------------------------------------------
+
+SQLITE_URL = "sqlite+aiosqlite://"
+
+
+@pytest_asyncio.fixture
+async def db_session() -> AsyncSession:
+    """
+    Yield a throwaway SQLite AsyncSession with all tables created.
+    Each test gets a fresh in-memory DB.
+    """
+    engine = create_async_engine(SQLITE_URL, echo=False)
+
+    # Create all mapped tables in the in-memory SQLite DB
+    async with engine.begin() as conn:
+        await conn.run_sync(Base.metadata.create_all)
+
+    session_factory = async_sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)
+
+    async with session_factory() as session:
+        yield session
+
+    # Teardown
+    async with engine.begin() as conn:
+        await conn.run_sync(Base.metadata.drop_all)
+    await engine.dispose()
+
+
+# ---------------------------------------------------------------------------
+# MongoDB mock engine for unit tests
+# ---------------------------------------------------------------------------
+
+@pytest_asyncio.fixture
+async def mock_mongo_db():
+    """Return a mongomock-motor database instance."""
+    import mongomock_motor
+
+    client = mongomock_motor.AsyncMongoMockClient()
+    db = client["test_db"]
+    yield db
+    client.close()
+
+
+# ---------------------------------------------------------------------------
+# Marks
+# ---------------------------------------------------------------------------
+
+def pytest_configure(config):
+    config.addinivalue_line(
+        "markers",
+        "integration: marks tests as requiring a live database (deselect with -m 'not integration')",
+    )
diff --git a/tests/test_mongodb_client.py b/tests/test_mongodb_client.py
new file mode 100644
index 0000000..7b86eb5
--- /dev/null
+++ b/tests/test_mongodb_client.py
@@ -0,0 +1,81 @@
+"""
+Unit tests for the MongoDB client layer.
+
+Uses mongomock-motor â€” no live MongoDB connection required.
+"""
+
+from __future__ import annotations
+
+import os
+
+import pytest
+
+pytestmark = pytest.mark.asyncio
+
+
+# ---------------------------------------------------------------------------
+# MongoDB client singleton tests
+# ---------------------------------------------------------------------------
+
+
+class TestMongoDBClient:
+    async def test_connect_returns_motor_database(self, mock_mongo_db):
+        """connect_mongodb() should return a motor Database-like object."""
+        # The mock_mongo_db fixture returns a mongomock database;
+        # verify it has the expected collection API
+        assert hasattr(mock_mongo_db, "__getitem__")
+        col = mock_mongo_db["news_with_sentiment"]
+        assert col is not None
+
+    async def test_collections_constant_contains_expected_keys(self):
+        """COLLECTIONS dict should expose all expected collection names."""
+        from app.base.utils.mongodb import COLLECTIONS
+
+        expected = {"stock_lists", "news", "news_with_sentiment", "price_histories", "ticker_infos"}
+        assert expected == set(COLLECTIONS.keys())
+
+    async def test_collections_values_are_strings(self):
+        from app.base.utils.mongodb import COLLECTIONS
+
+        for key, value in COLLECTIONS.items():
+            assert isinstance(value, str), f"COLLECTIONS['{key}'] should be a string"
+
+    async def test_insert_and_find_via_mock(self, mock_mongo_db):
+        """Basic smoke test: insert a doc and retrieve it via mongomock."""
+        col = mock_mongo_db["price_histories"]
+        doc = {"ticker": "AAPL", "history": [{"time": "2024-01-01", "close": 185.0}]}
+        await col.insert_one(doc)
+
+        found = await col.find_one({"ticker": "AAPL"})
+        assert found is not None
+        assert found["ticker"] == "AAPL"
+        assert len(found["history"]) == 1
+
+    async def test_insert_many_and_count(self, mock_mongo_db):
+        col = mock_mongo_db["news_with_sentiment"]
+        docs = [{"title": f"News {i}", "overall_sentiment_score": 0.1 * i} for i in range(5)]
+        await col.insert_many(docs)
+
+        count = await col.count_documents({})
+        assert count == 5
+
+    async def test_find_with_filter(self, mock_mongo_db):
+        col = mock_mongo_db["ticker_infos"]
+        await col.insert_many([
+            {"ticker": "AAPL", "sector": "Tech"},
+            {"ticker": "XOM", "sector": "Energy"},
+        ])
+        found = await col.find_one({"ticker": "XOM"})
+        assert found["sector"] == "Energy"
+
+    async def test_db_name_from_env_var(self, monkeypatch):
+        """The correct database name is selected based on ENV env var."""
+        # We can't easily re-import the singleton, but we can verify the
+        # logic independently.
+        monkeypatch.setenv("ENV", "production")
+        db_name = "production" if os.environ.get("ENV") == "production" else "develop"
+        assert db_name == "production"
+
+        monkeypatch.setenv("ENV", "development")
+        db_name = "production" if os.environ.get("ENV") == "production" else "develop"
+        assert db_name == "develop"
diff --git a/tests/test_postgres_repository.py b/tests/test_postgres_repository.py
new file mode 100644
index 0000000..a68fbae
--- /dev/null
+++ b/tests/test_postgres_repository.py
@@ -0,0 +1,142 @@
+"""
+Unit tests for the Postgres repository layer.
+
+Uses an in-memory SQLite database (via aiosqlite) â€” no live Postgres required.
+
+Note: SQLite does not support server-side gen_random_uuid(), so test fixtures
+supply UUIDs explicitly.
+"""
+
+from __future__ import annotations
+
+import pytest
+
+from app.db.repository import (
+    BacktestStatRepository,
+    TradeActionRepository,
+)
+
+pytestmark = pytest.mark.asyncio
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+def _backtest_payload(**overrides) -> dict:
+    return {
+        "ticker": "AAPL",
+        "strategy": "macd_1",
+        "period": "1y",
+        "interval": "1d",
+        "sharpe_ratio": 1.5,
+        "return_percentage": 12.3,
+        "win_rate": 55.0,
+        "trade_count": 42,
+        "notifications_on": True,
+        # omit other optional fields â€” they are nullable
+        **overrides,
+    }
+
+
+_trade_id_counter = 0
+
+def _trade_action_payload(backtest_id: int, dt: str = "2024-01-01T00:00:00") -> dict:
+    global _trade_id_counter
+    _trade_id_counter += 1
+    return {
+        "id": _trade_id_counter,
+        "backtest_id": backtest_id,
+        "datetime": dt,
+        "trade_action": "buy",
+        "price": 150.0,
+        "pnl": 5.0,
+        "return_pct": 3.3,
+    }
+
+
+# ---------------------------------------------------------------------------
+# BacktestStatRepository
+# ---------------------------------------------------------------------------
+
+
+class TestBacktestStatRepository:
+    async def test_insert_creates_record(self, db_session):
+        repo = BacktestStatRepository(db_session)
+        data = _backtest_payload()
+        stat = await repo.insert(data)
+        assert stat is not None
+        assert stat.ticker == "AAPL"
+
+    async def test_insert_returns_correct_id(self, db_session):
+        repo = BacktestStatRepository(db_session)
+        stat = await repo.insert(_backtest_payload(id=101))
+        assert stat.id == 101
+
+    async def test_upsert_creates_when_missing(self, db_session):
+        repo = BacktestStatRepository(db_session)
+        data = _backtest_payload()
+        stat = await repo.upsert(data)
+        assert stat is not None
+        assert stat.id is not None
+
+    async def test_upsert_updates_when_existing(self, db_session):
+        repo = BacktestStatRepository(db_session)
+        stat = await repo.insert(_backtest_payload(ticker="NVDA", sharpe_ratio=1.0))
+
+        updated = await repo.upsert(_backtest_payload(ticker="NVDA", sharpe_ratio=2.5))
+        assert updated is not None
+        assert updated.sharpe_ratio == 2.5
+        assert updated.id == stat.id
+
+    async def test_insert_multiple_records(self, db_session):
+        repo = BacktestStatRepository(db_session)
+        for i in range(3):
+            stat = await repo.insert(_backtest_payload(ticker=f"TICK{i}"))
+            assert stat is not None
+            assert stat.ticker == f"TICK{i}"
+
+
+# ---------------------------------------------------------------------------
+# TradeActionRepository
+# ---------------------------------------------------------------------------
+
+
+class TestTradeActionRepository:
+    async def test_insert_many_returns_all_records(self, db_session):
+        repo = TradeActionRepository(db_session)
+        # First we need a backtest stat in the DB (FK constraint)
+        stat = await BacktestStatRepository(db_session).insert(_backtest_payload())
+        stat_id = stat.id
+
+        records = [
+            _trade_action_payload(stat_id, f"2024-01-0{i + 1}T00:00:00")
+            for i in range(3)
+        ]
+        saved = await repo.insert_many(records)
+        assert len(saved) == 3
+
+    async def test_insert_many_empty_list_returns_empty(self, db_session):
+        repo = TradeActionRepository(db_session)
+        result = await repo.insert_many([])
+        assert result == []
+
+    async def test_get_latest_for_strategy_returns_newest(self, db_session):
+        stat = await BacktestStatRepository(db_session).insert(_backtest_payload())
+        stat_id = stat.id
+
+        ta_repo = TradeActionRepository(db_session)
+        await ta_repo.insert_many([
+            _trade_action_payload(stat_id, "2024-01-01T00:00:00"),
+            _trade_action_payload(stat_id, "2024-03-01T00:00:00"),
+            _trade_action_payload(stat_id, "2024-02-01T00:00:00"),
+        ])
+
+        latest = await ta_repo.get_latest_for_strategy(stat_id)
+        assert latest is not None
+        assert latest.datetime == "2024-03-01T00:00:00"
+
+    async def test_get_latest_for_strategy_returns_none_when_empty(self, db_session):
+        repo = TradeActionRepository(db_session)
+        result = await repo.get_latest_for_strategy(999)
+        assert result is None
